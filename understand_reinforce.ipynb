{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9be3ed82e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, 'env'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyApproximator(Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(16, activation='relu', name='dense1')\n",
    "        self.d2 = Dense(8, activation='relu', name='dense2')\n",
    "       # self.d3 = Dense(32, activation='relu', name='dense3')\n",
    "       # self.d4 = Dense(16, activation='relu', name='dense4')\n",
    "        self.d5 = Dense(2, activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "     #   x = self.d3(x)\n",
    "     #   x = self.d4(x)\n",
    "        return self.d5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(states):\n",
    "    return tf.nn.softmax(policy_approximator(states))\n",
    "\n",
    "def log_policy(states):\n",
    "    return tf.nn.log_softmax(policy_approximator(states))\n",
    "\n",
    "def get_action_probs(states):\n",
    "    log_action_probs = tf.squeeze(log_policy(states), 0)\n",
    "    action_probs = tf.squeeze(policy(states), 0)\n",
    "    return log_action_probs, action_probs\n",
    "\n",
    "def get_cumulative_rewards(rewards,  gamma=0.99):\n",
    "    cumulative_returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for i in range(len(rewards) - 1, -1, -1):\n",
    "        G = gamma * G + rewards[i]\n",
    "        cumulative_returns[i] = G\n",
    "    return cumulative_returns\n",
    "\n",
    "def compute_objective(log_action_probs, actions, cumulative_returns):\n",
    "    log_action_taken_probs = tf.gather_nd(log_action_probs, tf.stack([tf.range(log_action_probs.shape[0]), actions], axis=1))\n",
    "    J = tf.math.reduce_mean(tf.math.multiply(log_action_taken_probs, cumulative_returns))\n",
    "    return J\n",
    "\n",
    "def compute_entropy(action_probs, log_action_probs):\n",
    "    return - tf.math.reduce_sum(tf.math.multiply(action_probs, log_action_probs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions and rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = np.asarray([], dtype='float32'), [], []\n",
    "    s = env.reset().astype('float32')\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities and log probabilities\n",
    "        log_action_probs, action_probs = get_action_probs(s)\n",
    "        log_action_probs = log_action_probs.numpy().reshape(2,)\n",
    "        action_probs = action_probs.numpy().reshape(2,)\n",
    "        # choose an action\n",
    "        action_t = np.random.choice([0, 1], p=action_probs)\n",
    "        # perform a step\n",
    "        new_s, r, done, info = env.step(action_t)\n",
    "\n",
    "        # record session history to train later\n",
    "        states = np.concatenate((states, s), axis=0)\n",
    "        actions.append(action_t)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s.astype('float32')\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states.reshape(-1, s.shape[0]), actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "policy_approximator = PolicyApproximator(action_dim=env.action_space.n)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "\n",
    "    # Gradient descent step\n",
    "    with tf.GradientTape() as tape:\n",
    "        # cast everything into tensorflow tensors\n",
    "        states = tf.Variable(states, dtype='float32', name='states')\n",
    "        actions = tf.Variable(actions, dtype='int32', name='actions')\n",
    "        cumulative_returns = get_cumulative_rewards(rewards, gamma)\n",
    "        cumulative_returns = tf.Variable(cumulative_returns, dtype='float32', name='cumulative_returns')\n",
    "        \n",
    "        # Compute action probs\n",
    "        log_action_probs, action_probs = get_action_probs(states)\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy = compute_entropy(log_action_probs, action_probs)\n",
    "        \n",
    "        # Compute objective\n",
    "        J = compute_objective(log_action_probs, actions, cumulative_returns)\n",
    "        loss = -J - entropy_coef * entropy\n",
    "        \n",
    "    \n",
    "    gradients = tape.gradient(loss, policy_approximator.trainable_variables, )\n",
    "    optimizer.apply_gradients(zip(gradients, policy_approximator.trainable_variables))        \n",
    "\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number 0 and the reward is 35.0\n",
      "Episode number 100 and the reward is 23.87\n",
      "Episode number 200 and the reward is 25.17\n",
      "Episode number 300 and the reward is 26.31\n",
      "Episode number 400 and the reward is 34.98\n",
      "Episode number 500 and the reward is 37.06\n",
      "Episode number 600 and the reward is 50.49\n",
      "Episode number 700 and the reward is 58.51\n",
      "Episode number 800 and the reward is 71.7\n",
      "Episode number 900 and the reward is 78.92\n",
      "Episode number 1000 and the reward is 127.07\n",
      "Episode number 1100 and the reward is 153.26\n",
      "Episode number 1200 and the reward is 137.27\n",
      "Episode number 1300 and the reward is 143.98\n",
      "Episode number 1400 and the reward is 172.35\n",
      "Episode number 1500 and the reward is 143.38\n",
      "Episode number 1600 and the reward is 148.83\n",
      "Episode number 1700 and the reward is 256.84\n",
      "Episode number 1800 and the reward is 228.65\n",
      "Episode number 1900 and the reward is 262.15\n",
      "Episode number 2000 and the reward is 215.34\n",
      "Episode number 2100 and the reward is 241.93\n",
      "Episode number 2200 and the reward is 257.82\n",
      "Episode number 2300 and the reward is 267.76\n",
      "Episode number 2400 and the reward is 169.78\n",
      "Episode number 2500 and the reward is 211.43\n",
      "Episode number 2600 and the reward is 249.29\n",
      "Episode number 2700 and the reward is 214.33\n",
      "Episode number 2800 and the reward is 215.65\n",
      "Episode number 2900 and the reward is 203.05\n",
      "Episode number 3000 and the reward is 213.51\n",
      "Episode number 3100 and the reward is 270.8\n",
      "Episode number 3200 and the reward is 244.04\n",
      "Episode number 3300 and the reward is 259.54\n",
      "Episode number 3400 and the reward is 269.1\n",
      "Episode number 3500 and the reward is 163.85\n",
      "Episode number 3600 and the reward is 214.32\n",
      "Episode number 3700 and the reward is 220.68\n",
      "Episode number 3800 and the reward is 246.55\n",
      "Episode number 3900 and the reward is 219.43\n",
      "Episode number 4000 and the reward is 200.49\n",
      "Episode number 4100 and the reward is 159.86\n",
      "Episode number 4200 and the reward is 188.8\n",
      "Episode number 4300 and the reward is 115.26\n",
      "Episode number 4400 and the reward is 113.51\n",
      "Episode number 4500 and the reward is 213.35\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "global_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    rewards = train_on_session(*generate_session()[0:3]) # generate new sessions\n",
    "    global_rewards.append(rewards)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode number {} and the reward is {}\".format(i,np.mean(global_rewards[-100:])))\n",
    "\n",
    "    if np.mean(global_rewards[-100:]) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_approximator.save_weights('model_weights/trained_policy_REINFORCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.24841.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

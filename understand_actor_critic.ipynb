{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f288895b160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueApproximator(Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(32, activation='relu', name='dense1')\n",
    "        self.d2 = Dense(16, activation='relu', name='dense2')\n",
    "        self.d3_policy = Dense(2, activation='relu', name='dense_policy')\n",
    "        self.d3_value = Dense(1, activation='linear', name='dense_value')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        policy = self.d3_policy(x)\n",
    "        value = self.d3_value(x)\n",
    "        return [value, policy]\n",
    "        \n",
    "    def model(self):\n",
    "       # x = Input(shape=(4, ))\n",
    "        x = Input(shape=(4,))\n",
    "        return Model(inputs=x, outputs=self(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 16)           528         dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_value (Dense)             (None, 1)            17          dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_policy (Dense)            (None, 2)            34          dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 739\n",
      "Trainable params: 739\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "policy_value_approximator = PolicyValueApproximator(action_dim=n_actions)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "policy_value_approximator.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(states):\n",
    "    return tf.nn.softmax(policy_value_approximator(states)[1])\n",
    "\n",
    "def log_policy(states):\n",
    "    return tf.nn.log_softmax(policy_value_approximator(states)[1])\n",
    "\n",
    "def get_action_probs(states):\n",
    "    log_action_probs = tf.squeeze(log_policy(states), 0)\n",
    "    action_probs = tf.squeeze(policy(states), 0)\n",
    "    return log_action_probs, action_probs\n",
    "\n",
    "def compute_entropy(action_probs, log_action_probs):\n",
    "    return - tf.math.reduce_sum(tf.math.multiply(action_probs, log_action_probs))\n",
    "\n",
    "def compute_policy_objective(log_action_probs, actions, rewards):\n",
    "  #log_action_taken_probs = tf.gather_nd(log_action_probs, tf.stack([tf.range(log_action_probs.shape[0]), actions], axis=1))\n",
    "    log_action_taken_probs = log_action_probs[a]\n",
    "    J = tf.math.reduce_mean(tf.math.multiply(log_action_taken_probs, rewards))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state_previous, state, action, reward, gamma=0.99, entropy_coef=1e-2):\n",
    "    \n",
    "    # Environment step\n",
    "    \n",
    "    # Gradient descent step  \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s_prev = tf.Variable(state_previous, dtype='float32', name='state_prev')\n",
    "        s = tf.Variable(state, dtype='float32', name='state')\n",
    "        a = tf.Variable(action, dtype='int32', name='action')\n",
    "        r = tf.Variable(reward, dtype='float32', name='reward')\n",
    "        \n",
    "        # Get policy and value functions\n",
    "        v_s, p_s = policy_value_approximator(s)\n",
    "        \n",
    "        # Get value function for the previous state\n",
    "        v_s_prev = policy_value_approximator(s_prev)[0]\n",
    "        \n",
    "        # Compute action probs\n",
    "        log_action_probs, action_probs = get_action_probs(s)\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy = compute_entropy(log_action_probs, action_probs)\n",
    "        \n",
    "        # Compute policy objective\n",
    "        J = compute_policy_objective(log_action_probs, a, r)\n",
    "        policy_loss = -J - entropy_coef * entropy\n",
    "        \n",
    "        # Compute Bellman error\n",
    "        delta = r + gamma * v_s - v_s_prev\n",
    "        \n",
    "        # Compute MSVE \n",
    "        MSVE = tf.math.square(delta)\n",
    "    \n",
    "    # Optimise the net and the policy output weights\n",
    "    gradients_policy = tape.gradient(policy_loss, policy_value_approximator.trainable_variables[:6])\n",
    "   # for i, gradient in enumerate(gradients_policy):\n",
    "   #     gradients_policy[i] = delta * gamma * gradient\n",
    "    optimizer.apply_gradients(zip(gradients_policy, policy_value_approximator.trainable_variables[:6]))      \n",
    "\n",
    "    # Optimise the net and the policy output weights\n",
    "    gradients_value = tape.gradient(MSVE, policy_value_approximator.trainable_variables[6:8])\n",
    "   # for i, gradient in enumerate(gradients_value):\n",
    "   #     gradients_value[i] = delta * gradient\n",
    "    optimizer.apply_gradients(zip(gradients_value, policy_value_approximator.trainable_variables[6:8]))      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    # arrays to record session\n",
    "    states, actions, rewards = np.asarray([], dtype='float32'), [], []\n",
    "\n",
    "    s_prev = env.reset().astype('float32')\n",
    "    \n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities and log probabilities\n",
    "        log_action_probs, action_probs = get_action_probs(np.reshape(s_prev, (1, 4)))\n",
    "        log_action_probs = log_action_probs.numpy().reshape(2,)\n",
    "        action_probs = action_probs.numpy().reshape(2,)\n",
    "        # choose an action\n",
    "        a = np.random.choice([0, 1], p=action_probs)\n",
    "        # perform a step\n",
    "        s, r, done, info = env.step(a)\n",
    "        \n",
    "        # perform a train step\n",
    "        train_step(state_previous=np.reshape(s_prev, (1, 4)), state=np.reshape(s, (1, 4)), action=a, reward=r)\n",
    "        \n",
    "        # record session history to train later\n",
    "        states = np.concatenate((states, s), axis=0)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s_prev = s.astype('float32')\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states.reshape(-1, s.shape[0]), actions, np.sum(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "policy_value_approximator = PolicyValueApproximator(action_dim=n_actions)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number 0 and the reward is 31.0\n",
      "Episode number 100 and the reward is 9.75\n",
      "Episode number 200 and the reward is 9.48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-bf94ec1531a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mglobal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-7538b6b8a058>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# perform a train step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# record session history to train later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-fec748998011>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(state_previous, state, action, reward, gamma, entropy_coef)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Get policy and value functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    233\u001b[0m                         shape=None):\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2554\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2556\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1404\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1550\u001b[0m               \u001b[0mshared_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m               \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m               graph_mode=self._in_graph_mode)\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         if (self._in_graph_mode and initial_value is not None and\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36meager_safe_variable_handle\u001b[0;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[1;32m    224\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   return variable_handle_from_shape_and_dtype(\n\u001b[0;32m--> 226\u001b[0;31m       shape, dtype, shared_name, name, graph_mode, initial_value)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvariable_handle_from_shape_and_dtype\u001b[0;34m(shape, dtype, shared_name, name, graph_mode, extra_handle_data)\u001b[0m\n\u001b[1;32m    164\u001b[0m                                                   \u001b[0mshared_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                                                   \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                                   container=container)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Tensor._handle_data contains information for the shape-inference code to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mvar_handle_op\u001b[0;34m(dtype, shape, container, shared_name, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   1430\u001b[0m         \u001b[0;34m\"VarHandleOp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m                        shared_name=shared_name, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3358\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3360\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m   def _create_op_internal(\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3412\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m     \u001b[0mnode_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_NodeDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[0minput_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_NodeDef\u001b[0;34m(op_type, name, device, attrs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m       \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    _, _, rewards = generate_session() # generate new sessions\n",
    "    global_rewards.append(rewards)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode number {} and the reward is {}\".format(i, np.mean(global_rewards[-100:])))\n",
    "    \n",
    "    if np.mean(global_rewards[-100:]) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_approximator.save_weights('model_weights/trained_policy_actor_critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos_ac\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.24841.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_ac/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f288895b160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueApproximator(Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(32, activation='relu', name='dense1')\n",
    "        self.d2 = Dense(16, activation='relu', name='dense2')\n",
    "        self.d3_policy = Dense(2, activation='relu', name='dense_policy')\n",
    "        self.d3_value = Dense(1, activation='linear', name='dense_value')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        policy = self.d3_policy(x)\n",
    "        value = self.d3_value(x)\n",
    "        return [value, policy]\n",
    "        \n",
    "    def model(self):\n",
    "       # x = Input(shape=(4, ))\n",
    "        x = Input(shape=(4,))\n",
    "        return Model(inputs=x, outputs=self(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 16)           528         dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_value (Dense)             (None, 1)            17          dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_policy (Dense)            (None, 2)            34          dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 739\n",
      "Trainable params: 739\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "policy_value_approximator = PolicyValueApproximator(action_dim=n_actions)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "policy_value_approximator.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(states):\n",
    "    return tf.nn.softmax(policy_value_approximator(states)[1])\n",
    "\n",
    "def log_policy(states):\n",
    "    return tf.nn.log_softmax(policy_value_approximator(states)[1])\n",
    "\n",
    "def get_action_probs(states):\n",
    "    log_action_probs = tf.squeeze(log_policy(states), 0)\n",
    "    action_probs = tf.squeeze(policy(states), 0)\n",
    "    return log_action_probs, action_probs\n",
    "\n",
    "def compute_entropy(action_probs, log_action_probs):\n",
    "    return - tf.math.reduce_sum(tf.math.multiply(action_probs, log_action_probs))\n",
    "\n",
    "def compute_policy_objective(log_action_probs, actions, rewards):\n",
    "  #log_action_taken_probs = tf.gather_nd(log_action_probs, tf.stack([tf.range(log_action_probs.shape[0]), actions], axis=1))\n",
    "    log_action_taken_probs = log_action_probs[a]\n",
    "    J = tf.math.reduce_mean(tf.math.multiply(log_action_taken_probs, rewards))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state_previous, state, action, reward, gamma=0.99, entropy_coef=1e-2):\n",
    "    \n",
    "    # Environment step\n",
    "    \n",
    "    # Gradient descent step  \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s_prev = tf.Variable(state_previous, dtype='float32', name='state_prev')\n",
    "        s = tf.Variable(state, dtype='float32', name='state')\n",
    "        a = tf.Variable(action, dtype='int32', name='action')\n",
    "        r = tf.Variable(reward, dtype='float32', name='reward')\n",
    "        \n",
    "        # Get policy and value functions\n",
    "        v_s, p_s = policy_value_approximator(s)\n",
    "        \n",
    "        # Get value function for the previous state\n",
    "        v_s_prev = policy_value_approximator(s_prev)[0]\n",
    "        \n",
    "        # Compute action probs\n",
    "        log_action_probs, action_probs = get_action_probs(s)\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy = compute_entropy(log_action_probs, action_probs)\n",
    "        \n",
    "        # Compute policy objective\n",
    "        J = compute_policy_objective(log_action_probs, a, r)\n",
    "        policy_loss = -J - entropy_coef * entropy\n",
    "        \n",
    "        # Compute Bellman error\n",
    "        delta = r + gamma * v_s - v_s_prev\n",
    "        \n",
    "        # Compute MSVE \n",
    "        MSVE = tf.math.square(delta)\n",
    "    \n",
    "    # Optimise the net and the policy output weights\n",
    "    gradients_policy = tape.gradient(policy_loss, policy_value_approximator.trainable_variables[:6])\n",
    "   # for i, gradient in enumerate(gradients_policy):\n",
    "   #     gradients_policy[i] = delta * gamma * gradient\n",
    "    optimizer.apply_gradients(zip(gradients_policy, policy_value_approximator.trainable_variables[:6]))      \n",
    "\n",
    "    # Optimise the net and the policy output weights\n",
    "    gradients_value = tape.gradient(MSVE, policy_value_approximator.trainable_variables[6:8])\n",
    "   # for i, gradient in enumerate(gradients_value):\n",
    "   #     gradients_value[i] = delta * gradient\n",
    "    optimizer.apply_gradients(zip(gradients_value, policy_value_approximator.trainable_variables[6:8]))      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    # arrays to record session\n",
    "    states, actions, rewards = np.asarray([], dtype='float32'), [], []\n",
    "\n",
    "    s_prev = env.reset().astype('float32')\n",
    "    \n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities and log probabilities\n",
    "        log_action_probs, action_probs = get_action_probs(np.reshape(s_prev, (1, 4)))\n",
    "        log_action_probs = log_action_probs.numpy().reshape(2,)\n",
    "        action_probs = action_probs.numpy().reshape(2,)\n",
    "        # choose an action\n",
    "        a = np.random.choice([0, 1], p=action_probs)\n",
    "        # perform a step\n",
    "        s, r, done, info = env.step(a)\n",
    "        \n",
    "        # perform a train step\n",
    "        train_step(state_previous=np.reshape(s_prev, (1, 4)), state=np.reshape(s, (1, 4)), action=a, reward=r)\n",
    "        \n",
    "        # record session history to train later\n",
    "        states = np.concatenate((states, s), axis=0)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s_prev = s.astype('float32')\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states.reshape(-1, s.shape[0]), actions, rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "policy_value_approximator = PolicyValueApproximator(action_dim=n_actions)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number 0 and the reward is 1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-959295df41a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode number {} and the reward is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You Win!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "global_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    _, _, rewards = generate_session() # generate new sessions\n",
    "    global_rewards.append(rewards)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode number {} and the reward is {}\".format(i, np.mean(global_rewards[-100:])))\n",
    "    \n",
    "    if np.mean(global_rewards[-100:]) >= 300.0:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_approximator.save_weights('model_weights/trained_policy_actor_critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos_ac\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.24841.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_ac/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

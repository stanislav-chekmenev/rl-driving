{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f00486d70f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueApproximator(Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(32, activation='relu', name='dense1')\n",
    "        self.d2 = Dense(16, activation='relu', name='dense2')\n",
    "        self.d3_policy = Dense(2, activation='relu', name='dense_policy')\n",
    "        self.d3_value = Dense(1, activation='linear', name='dense_value')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        policy = self.d3_policy(x)\n",
    "        value = self.d3_value(x)\n",
    "        return [value, policy]\n",
    "        \n",
    "    def model(self):\n",
    "       # x = Input(shape=(4, ))\n",
    "        x = Input(shape=(4,))\n",
    "        return Model(inputs=x, outputs=self(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 16)           528         dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_value (Dense)             (None, 1)            17          dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_policy (Dense)            (None, 2)            34          dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 739\n",
      "Trainable params: 739\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "policy_value_approximator = PolicyValueApproximator(action_dim=n_actions)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "policy_value_approximator.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=143, shape=(1, 1), dtype=float32, numpy=array([[-0.00542283]], dtype=float32)>,\n",
       " <tf.Tensor: id=139, shape=(1, 2), dtype=float32, numpy=array([[0.0061877, 0.       ]], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_value_approximator(np.reshape(env.reset().astype('float32'), (1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense1/kernel:0' shape=(4, 32) dtype=float32, numpy=\n",
       " array([[-0.21471545, -0.30401823,  0.2201537 , -0.13353506,  0.01762212,\n",
       "         -0.03336084, -0.14911483, -0.00531427,  0.15315236,  0.4068918 ,\n",
       "         -0.16688442,  0.08944829, -0.17637153,  0.36296695, -0.0882881 ,\n",
       "          0.30591798,  0.28081745, -0.17722386, -0.34958377, -0.05544568,\n",
       "          0.08570537,  0.08991198,  0.39227292, -0.02545653,  0.35692555,\n",
       "          0.37560666, -0.27415413, -0.30635208,  0.04854113, -0.39952987,\n",
       "          0.12690419, -0.375425  ],\n",
       "        [ 0.32559037, -0.34090972,  0.07306491,  0.20169626,  0.22415589,\n",
       "         -0.14581263,  0.33016375, -0.07534275, -0.26816827,  0.11105263,\n",
       "         -0.36362088,  0.07719202, -0.02258976, -0.15734988,  0.23373282,\n",
       "         -0.16440609,  0.12713397,  0.36122423,  0.10219144,  0.00365867,\n",
       "          0.40643978, -0.3809655 ,  0.05943088, -0.15794407, -0.14035696,\n",
       "          0.08797608,  0.02872825,  0.06389271, -0.29589406, -0.40435645,\n",
       "         -0.38568187, -0.07331416],\n",
       "        [-0.17957737,  0.20557769, -0.05834412, -0.08626017,  0.2924834 ,\n",
       "          0.1990144 , -0.01352328,  0.04120309,  0.01416311,  0.16657487,\n",
       "          0.10815834,  0.11675476, -0.24674746, -0.01567947, -0.20660235,\n",
       "         -0.01499026, -0.4012878 , -0.16369359, -0.09682938, -0.1863712 ,\n",
       "          0.15570055, -0.05620088, -0.24315636,  0.01256882, -0.15511364,\n",
       "          0.13637832,  0.39290306, -0.05156932, -0.29326883,  0.18990219,\n",
       "         -0.07774862,  0.23434158],\n",
       "        [ 0.03930685,  0.01728214, -0.37430948,  0.11663236, -0.06391054,\n",
       "         -0.38863456,  0.0917473 ,  0.23224755,  0.22579688,  0.34871832,\n",
       "         -0.2680464 ,  0.38507447, -0.242624  ,  0.2967089 , -0.210774  ,\n",
       "          0.2626044 , -0.16432181, -0.04756042, -0.22096305,  0.35688278,\n",
       "         -0.32881054, -0.0445057 , -0.33890465, -0.25377378,  0.02680203,\n",
       "         -0.18376645,  0.3703118 , -0.3469554 , -0.06693207,  0.35908115,\n",
       "         -0.09676886,  0.18847407]], dtype=float32)>,\n",
       " <tf.Variable 'dense1/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.00118787,  0.00098176,  0.01295498,  0.00868309, -0.0095363 ,\n",
       "        -0.00370232, -0.00562339,  0.00032775,  0.00096543, -0.00181654,\n",
       "         0.00590136, -0.00344205, -0.00952945, -0.00038325,  0.00520929,\n",
       "        -0.00369785,  0.00134467,  0.00755168, -0.00250806, -0.00680927,\n",
       "        -0.00117103,  0.00208951, -0.00044174, -0.00525348, -0.00203181,\n",
       "        -0.00193819, -0.00191249, -0.00866802, -0.00157817,  0.0003078 ,\n",
       "        -0.00224277, -0.00317711], dtype=float32)>,\n",
       " <tf.Variable 'dense2/kernel:0' shape=(32, 16) dtype=float32, numpy=\n",
       " array([[-9.78686139e-02, -2.33339563e-01, -1.84106365e-01,\n",
       "         -1.57346740e-01,  3.14137459e-01, -9.64829624e-02,\n",
       "          1.81962132e-01, -2.43995011e-01,  2.28309587e-01,\n",
       "          4.98159826e-02, -3.11022222e-01, -6.78032339e-02,\n",
       "         -1.68560773e-01,  1.30606264e-01,  3.05580378e-01,\n",
       "          2.42088735e-01],\n",
       "        [ 1.95261374e-01,  3.13637286e-01, -2.06211299e-01,\n",
       "         -2.57444799e-01, -1.80868655e-01,  3.30644876e-01,\n",
       "         -2.61122644e-01,  1.93019733e-01,  7.27726817e-02,\n",
       "         -4.80079129e-02,  1.13505170e-01,  2.90030688e-01,\n",
       "         -2.79816657e-01, -3.09097052e-01, -1.52686760e-01,\n",
       "         -2.14401349e-01],\n",
       "        [-1.23778442e-02,  2.51788676e-01, -3.12096238e-01,\n",
       "          2.64620066e-01,  2.35606562e-02,  1.22007370e-01,\n",
       "          1.18783474e-01, -2.21065611e-01,  2.38889232e-01,\n",
       "          2.15778381e-01,  9.51272473e-02,  2.91648149e-01,\n",
       "          3.24620726e-03, -1.93922415e-01,  1.71967186e-02,\n",
       "         -2.69365340e-01],\n",
       "        [ 1.73507422e-01, -2.80797273e-01, -2.03146040e-01,\n",
       "          2.20992088e-01,  1.16553627e-01,  1.65631801e-01,\n",
       "          2.64189959e-01,  3.06727529e-01,  2.85418272e-01,\n",
       "         -2.22895801e-01,  2.24774972e-01,  8.92237425e-02,\n",
       "         -2.82436777e-02, -2.22808555e-01,  2.00666651e-01,\n",
       "         -3.15069824e-01],\n",
       "        [ 3.32175046e-01,  2.12168079e-02, -1.89779252e-01,\n",
       "          1.05312038e-02, -2.44664133e-01,  7.74134547e-02,\n",
       "         -9.68951881e-02,  6.92692697e-02, -2.19676085e-02,\n",
       "         -1.06140077e-02,  3.34451944e-01, -2.41396010e-01,\n",
       "         -1.93098298e-04, -6.74245059e-02, -1.41621334e-02,\n",
       "         -7.42985085e-02],\n",
       "        [-6.63526282e-02,  1.83525637e-01, -2.31000036e-01,\n",
       "         -1.08186841e-01, -2.35287011e-01,  7.58421188e-03,\n",
       "         -2.01558173e-02,  5.47754616e-02,  2.31951829e-02,\n",
       "         -8.01267326e-02, -1.95748359e-01,  2.05614090e-01,\n",
       "          1.15648188e-01,  3.25489432e-01, -5.54347411e-02,\n",
       "          3.31956387e-01],\n",
       "        [ 1.09720036e-01, -8.12760368e-02, -2.31950283e-01,\n",
       "          3.37784350e-01, -3.19870422e-03, -1.17706828e-01,\n",
       "         -2.93106586e-01,  4.90035415e-02,  1.48300216e-01,\n",
       "         -1.03503466e-03,  6.08540438e-02, -2.89451517e-02,\n",
       "          1.00437529e-01, -3.12885523e-01, -2.61528492e-01,\n",
       "          2.83668309e-01],\n",
       "        [ 2.62417853e-01, -2.51753986e-01,  2.24123865e-01,\n",
       "          1.40060842e-01, -2.42669910e-01,  2.53544338e-02,\n",
       "         -2.72494614e-01, -2.45888337e-01,  8.22311044e-02,\n",
       "          2.50144750e-01, -1.77622899e-01, -2.70467877e-01,\n",
       "         -2.86318690e-01, -2.01655269e-01,  2.47663587e-01,\n",
       "          1.65472418e-01],\n",
       "        [-2.38252610e-01, -1.20857939e-01, -1.31898820e-01,\n",
       "         -1.20422311e-01,  2.15315521e-01, -2.86437571e-01,\n",
       "         -1.29975080e-02,  2.98508018e-01, -6.22600317e-04,\n",
       "          1.71223149e-01, -2.87855953e-01, -7.23664761e-02,\n",
       "          7.23273214e-03,  1.35906428e-01, -6.04516491e-02,\n",
       "         -1.82520971e-01],\n",
       "        [ 1.22878291e-01, -2.87165433e-01, -1.54017895e-01,\n",
       "         -1.02254361e-01, -2.49735102e-01,  1.68030247e-01,\n",
       "          3.17410201e-01, -1.12420090e-01, -3.43384355e-01,\n",
       "         -3.34719390e-01, -2.39901215e-01,  6.20474815e-02,\n",
       "          2.26475134e-01, -3.04884911e-01, -6.28245696e-02,\n",
       "         -1.79482937e-01],\n",
       "        [ 2.82468051e-01, -5.12116123e-03, -1.28487363e-01,\n",
       "         -3.62035006e-01, -9.92068797e-02, -2.81478882e-01,\n",
       "         -3.79605591e-02,  3.45956355e-01,  1.41215086e-01,\n",
       "          5.63959964e-03,  2.69104570e-01, -3.59867513e-01,\n",
       "          2.01176763e-01,  3.30963880e-01,  3.51240844e-01,\n",
       "          4.27939603e-03],\n",
       "        [-1.04065150e-01,  2.76582271e-01, -1.96696475e-01,\n",
       "          3.02698184e-02,  2.05442980e-02, -8.65848586e-02,\n",
       "          9.95069742e-03, -1.53836787e-01,  2.44831651e-01,\n",
       "          1.97232619e-01,  7.27648437e-02, -1.05759755e-01,\n",
       "          1.80093080e-01, -2.91294336e-01,  1.57554820e-01,\n",
       "         -8.57090950e-03],\n",
       "        [ 2.69246697e-01, -9.18469727e-02, -1.80328161e-01,\n",
       "         -3.58693600e-01,  5.94181307e-02, -1.93856105e-01,\n",
       "          2.33413115e-01, -3.03363323e-01, -8.72875229e-02,\n",
       "          1.95592970e-01,  3.60106856e-01,  1.88134555e-02,\n",
       "         -2.29659766e-01, -3.16800058e-01,  1.98770408e-03,\n",
       "         -2.91624874e-01],\n",
       "        [ 2.16361865e-01, -2.99684405e-01,  1.05520278e-01,\n",
       "         -3.09230208e-01,  8.58491883e-02, -9.19423625e-02,\n",
       "          1.56470031e-01,  3.06657016e-01, -4.85365689e-02,\n",
       "          3.07765175e-02, -1.43496618e-01,  2.77990907e-01,\n",
       "          1.32851094e-01, -8.24993402e-02, -9.41947699e-02,\n",
       "         -2.30010450e-02],\n",
       "        [ 3.27360213e-01, -1.75786674e-01,  1.15622640e-01,\n",
       "         -5.37345409e-02,  3.24100018e-01,  2.07820982e-01,\n",
       "          1.35262087e-01, -1.28940865e-01, -1.62071854e-01,\n",
       "         -1.80582643e-01, -2.89359450e-01,  1.15921944e-02,\n",
       "          2.03977436e-01,  7.41378684e-03,  1.72919691e-01,\n",
       "          2.39622965e-01],\n",
       "        [ 1.93251938e-01, -4.25583720e-02, -2.68338770e-01,\n",
       "          2.49087408e-01,  3.44914049e-01,  9.33637694e-02,\n",
       "          1.74051225e-02,  3.33241373e-02,  7.26390779e-02,\n",
       "          2.31982768e-01, -2.02339053e-01, -2.68535852e-01,\n",
       "         -1.06111370e-01, -2.98772752e-01,  2.30300836e-02,\n",
       "          2.78392404e-01],\n",
       "        [-1.94754884e-01,  2.13534757e-02,  1.98909432e-01,\n",
       "          1.69682592e-01, -2.19081081e-02, -6.35912940e-02,\n",
       "         -1.10833785e-02,  2.47786313e-01,  1.43505484e-01,\n",
       "         -1.71454310e-01,  2.69185305e-01,  1.57963052e-01,\n",
       "         -8.69264919e-03,  3.08844484e-02,  3.34475070e-01,\n",
       "          2.31744513e-01],\n",
       "        [ 1.56087711e-01, -2.96808541e-01, -3.34944189e-01,\n",
       "          2.52594531e-01, -2.97709167e-01,  3.41772377e-01,\n",
       "          3.48808281e-02, -9.27498937e-03, -5.89622930e-02,\n",
       "         -2.80247957e-01, -7.95048624e-02,  7.73717910e-02,\n",
       "         -8.64934549e-03,  2.98862278e-01,  2.88565576e-01,\n",
       "         -1.04578815e-01],\n",
       "        [-2.19663501e-01,  3.36362094e-01,  2.08310694e-01,\n",
       "          3.13480012e-02, -1.78280801e-01,  1.54812351e-01,\n",
       "         -2.41485730e-01, -2.14367539e-01, -2.32645631e-01,\n",
       "         -1.60569206e-01, -2.15616271e-01, -1.72139108e-01,\n",
       "         -3.17687482e-01, -1.77266553e-01, -3.23645473e-01,\n",
       "          1.30191967e-01],\n",
       "        [ 3.43596488e-01,  8.11820030e-02,  1.68779045e-01,\n",
       "          3.25572222e-01,  2.86114693e-01,  5.72457537e-02,\n",
       "          1.52743729e-02, -1.30169094e-01,  2.02325955e-01,\n",
       "          6.78340271e-02, -1.44001633e-01, -2.80116200e-01,\n",
       "         -1.16498418e-01, -1.24313228e-01,  3.50395858e-01,\n",
       "          1.22492522e-01],\n",
       "        [ 1.96527719e-01,  7.59918615e-02, -2.22194806e-01,\n",
       "         -5.34457378e-02, -2.95305271e-02, -6.15146942e-02,\n",
       "         -5.33578284e-02, -6.38317466e-02, -3.31878245e-01,\n",
       "          8.82512331e-03,  2.45001599e-01, -2.07054675e-01,\n",
       "          1.49016723e-01, -3.04509908e-01, -1.60634756e-01,\n",
       "          1.26903160e-02],\n",
       "        [ 7.03416839e-02,  2.71050066e-01, -1.82461202e-01,\n",
       "         -2.18272939e-01,  9.24528986e-02, -3.15783560e-01,\n",
       "          1.67253911e-02,  2.35902891e-01, -6.97650909e-02,\n",
       "          2.56362706e-01,  8.76413584e-02, -1.57381132e-01,\n",
       "          2.04056010e-01,  2.83705801e-01,  1.86426237e-01,\n",
       "         -1.39997661e-01],\n",
       "        [ 1.77337117e-02, -1.35708764e-01,  2.30666399e-02,\n",
       "          2.86895871e-01, -1.31026477e-01, -3.56547445e-01,\n",
       "         -1.70011446e-01, -2.29500622e-01,  1.38412639e-01,\n",
       "         -9.23358202e-02,  2.47427791e-01, -7.13207871e-02,\n",
       "          2.03090593e-01,  2.59012163e-01,  2.64105380e-01,\n",
       "          2.16751061e-02],\n",
       "        [-2.52126724e-01, -3.48711908e-01, -1.74159452e-01,\n",
       "         -7.85298720e-02, -3.68058980e-02, -1.66085288e-01,\n",
       "         -3.62257361e-02, -9.35890377e-02,  2.58914620e-01,\n",
       "          2.13032529e-01, -1.37264132e-01,  3.37246746e-01,\n",
       "         -3.17307740e-01, -2.75435656e-01, -2.98619628e-01,\n",
       "          3.31647813e-01],\n",
       "        [ 3.05288229e-02, -3.10616463e-01,  1.18070275e-01,\n",
       "         -3.29017162e-01,  1.33690402e-01, -9.69344974e-02,\n",
       "          2.24663943e-01,  1.55440092e-01,  1.75955743e-01,\n",
       "          7.64980018e-02,  2.52699375e-01, -2.63930738e-01,\n",
       "          3.36880028e-01, -2.20755562e-01, -2.01331139e-01,\n",
       "          9.03345048e-02],\n",
       "        [ 6.39172643e-02, -8.75145867e-02,  1.88826650e-01,\n",
       "          1.92079127e-01,  3.00432235e-01, -2.46772051e-01,\n",
       "          2.93391436e-01,  1.61062330e-01,  2.64790803e-01,\n",
       "         -2.40478367e-01, -9.96631235e-02,  1.71312869e-01,\n",
       "         -2.10003734e-01, -3.11371148e-01,  1.34645449e-02,\n",
       "         -1.23823822e-01],\n",
       "        [ 9.63374525e-02, -1.98723406e-01, -8.34225118e-02,\n",
       "          3.08538169e-01, -1.35243103e-01,  2.80960858e-01,\n",
       "          5.16327620e-02, -3.00256193e-01,  8.47184658e-03,\n",
       "          2.07977414e-01, -1.45987213e-01, -3.21353287e-01,\n",
       "          7.24302009e-02,  2.13331267e-01, -2.81460404e-01,\n",
       "          1.45745218e-01],\n",
       "        [-1.91854462e-01,  1.20301023e-02, -2.05509380e-01,\n",
       "         -1.74583927e-01,  2.64021405e-03,  2.98034161e-01,\n",
       "         -3.21589887e-01, -7.64987916e-02,  3.59797716e-01,\n",
       "          2.75615841e-01,  1.54086545e-01,  2.00948939e-01,\n",
       "         -1.65793642e-01, -6.24219049e-03, -3.82668041e-02,\n",
       "          3.47383022e-01],\n",
       "        [ 3.31677526e-01, -3.12522769e-01,  1.51886344e-02,\n",
       "         -2.65641630e-01,  2.47640565e-01, -1.85451090e-01,\n",
       "         -8.62588733e-02,  3.15642595e-01, -2.32013315e-01,\n",
       "         -1.20607026e-01, -1.74558666e-02, -1.22506909e-01,\n",
       "          2.05454916e-01,  1.62871048e-01, -1.52976170e-01,\n",
       "          3.50082070e-01],\n",
       "        [ 3.11123967e-01, -2.97344655e-01, -2.09336475e-01,\n",
       "         -1.02943115e-01, -5.00510819e-03, -5.62510379e-02,\n",
       "         -1.64093867e-01,  6.04361631e-02, -3.21386755e-01,\n",
       "         -7.22598583e-02,  7.38065341e-04, -4.49561179e-02,\n",
       "          1.88681781e-01,  4.53901924e-02,  1.03361323e-01,\n",
       "         -3.20706159e-01],\n",
       "        [-3.25243711e-01,  1.39027029e-01,  8.17178488e-02,\n",
       "          2.64343500e-01,  4.73822244e-02,  2.55960912e-01,\n",
       "          1.88303858e-01,  1.56882390e-01, -6.74799681e-02,\n",
       "         -2.47625917e-01,  3.32832873e-01, -3.54653895e-02,\n",
       "          3.48223388e-01,  3.42601478e-01, -3.46666843e-01,\n",
       "         -1.81374490e-01],\n",
       "        [-7.44055659e-02, -1.20469689e-01, -2.14749232e-01,\n",
       "         -2.12151363e-01,  8.60845149e-02,  1.55401424e-01,\n",
       "         -4.30959165e-02, -1.64280742e-01,  1.90866560e-01,\n",
       "         -2.54426330e-01, -1.38872325e-01, -2.05038488e-02,\n",
       "         -1.93995703e-02, -3.37351501e-01, -2.33786225e-01,\n",
       "          1.88248068e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense2/bias:0' shape=(16,) dtype=float32, numpy=\n",
       " array([-0.01481223,  0.00145023,  0.        ,  0.01942443, -0.00285133,\n",
       "         0.0640413 , -0.00402616, -0.00052577, -0.01523681, -0.00204678,\n",
       "        -0.01294173,  0.01501056,  0.0096386 , -0.00479632,  0.00193128,\n",
       "        -0.01503014], dtype=float32)>,\n",
       " <tf.Variable 'dense_policy/kernel:0' shape=(16, 2) dtype=float32, numpy=\n",
       " array([[ 0.13715148, -0.37263703],\n",
       "        [ 0.04651821,  0.40066886],\n",
       "        [ 0.5219976 ,  0.26386946],\n",
       "        [ 0.00649023, -0.54164743],\n",
       "        [-0.13704425,  0.22245204],\n",
       "        [ 0.17991543,  0.54734385],\n",
       "        [ 0.5752585 ,  0.5249611 ],\n",
       "        [ 0.09757984,  0.39391387],\n",
       "        [-0.2609873 , -0.44487327],\n",
       "        [ 0.49966502, -0.56471044],\n",
       "        [ 0.4481492 ,  0.45981693],\n",
       "        [ 0.15284401,  0.10725504],\n",
       "        [-0.10360333,  0.43907773],\n",
       "        [ 0.35345244,  0.06112599],\n",
       "        [-0.20471999,  0.14233613],\n",
       "        [-0.4532302 , -0.03526676]], dtype=float32)>,\n",
       " <tf.Variable 'dense_policy/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_value/kernel:0' shape=(16, 1) dtype=float32, numpy=\n",
       " array([[-0.12380581],\n",
       "        [ 0.0399653 ],\n",
       "        [ 0.27557737],\n",
       "        [ 0.2860811 ],\n",
       "        [-0.01307102],\n",
       "        [ 0.56449956],\n",
       "        [-0.02741066],\n",
       "        [ 0.36085203],\n",
       "        [-0.3044229 ],\n",
       "        [ 0.18872084],\n",
       "        [-0.28043908],\n",
       "        [ 0.5077745 ],\n",
       "        [ 0.4449931 ],\n",
       "        [-0.2595145 ],\n",
       "        [-0.274842  ],\n",
       "        [-0.57129455]], dtype=float32)>,\n",
       " <tf.Variable 'dense_value/bias:0' shape=(1,) dtype=float32, numpy=array([0.06417817], dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_value_approximator.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(states):\n",
    "    return tf.nn.softmax(policy_value_approximator(states)[1])\n",
    "\n",
    "def log_policy(states):\n",
    "    return tf.nn.log_softmax(policy_value_approximator(states)[1])\n",
    "\n",
    "def get_action_probs(states):\n",
    "    log_action_probs = tf.squeeze(log_policy(states), 0)\n",
    "    action_probs = tf.squeeze(policy(states), 0)\n",
    "    return log_action_probs, action_probs\n",
    "\n",
    "def compute_entropy(action_probs, log_action_probs):\n",
    "    return - tf.math.reduce_sum(tf.math.multiply(action_probs, log_action_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk = tf.keras.Sequential([...])\n",
    "head1 = tf.keras.Sequential([...])\n",
    "head2 = tf.keras.Sequential([...])\n",
    "\n",
    "path1 = tf.keras.Sequential([trunk, head1])\n",
    "path2 = tf.keras.Sequential([trunk, head2])\n",
    "\n",
    "# Train on primary dataset\n",
    "for x, y in main_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = path1(x)\n",
    "    loss = loss_fn_head1(prediction, y)\n",
    "  # Simultaneously optimize trunk and head1 weights.\n",
    "  gradients = tape.gradient(loss, path1.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, path1.trainable_variables))\n",
    "\n",
    "# Fine-tune second head, reusing the trunk\n",
    "for x, y in small_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = path2(x)\n",
    "    loss = loss_fn_head2(prediction, y)\n",
    "  # Only optimize head2 weights, not trunk weights\n",
    "  gradients = tape.gradient(loss, head2.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, head2.trainable_variables))\n",
    "\n",
    "# You can publish just the trunk computation for other people to reuse.\n",
    "tf.saved_model.save(trunk, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state_previous, state, action, reward, gamma=0.99, entropy_coef=1e-2):\n",
    "    \n",
    "    # Environment step\n",
    "    \n",
    "    # Gradient descent step  \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s_prev = tf.Variable(state_previous, dtype='float32', name='state_prev')\n",
    "        s = tf.Variable(state, dtype='float32', name='state')\n",
    "        a = tf.Variable(action, dtype='int32', name='action')\n",
    "        r = tf.Variable(reward, dtype='float32', name='reward')\n",
    "        \n",
    "        # Get the policy and value function prediction\n",
    "        v_s = policy_value_approximator(s)[0]\n",
    "        v_s_prev = policy_value_approximator(s_prev)[0]\n",
    "        \n",
    "        # Compute policy for a gradient update step\n",
    "        p_s = policy_value_approximator(s)[1]\n",
    "        \n",
    "        # Compute action probs\n",
    "        log_action_probs, action_probs = get_action_probs(s)\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy = compute_entropy(log_action_probs, action_probs)\n",
    "        \n",
    "        # Compute error\n",
    "        delta = r + gamma * v_s - v_s_prev\n",
    "    \n",
    "    # Optimise the net and the policy output weights\n",
    "    gradients_policy = tape.gradient(delta, policy_value_approximator.trainable_variables[:6])\n",
    "    optimizer.apply_gradients(zip(gradients_policy, policy_value_approximator.trainable_variables[:6]))      \n",
    "    \n",
    "    # Optimise the net and the policy output weights\n",
    "    gradients_value = tape.gradient(delta, policy_value_approximator.trainable_variables[6:8])\n",
    "    optimizer.apply_gradients(zip(gradients_value, policy_value_approximator.trainable_variables[6:8]))      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, p = policy_value_approximator(np.reshape(s, (1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    # arrays to record session\n",
    "    states, actions, rewards = np.asarray([], dtype='float32'), [], []\n",
    "\n",
    "    s_prev = env.reset().astype('float32')\n",
    "    \n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities and log probabilities\n",
    "        log_action_probs, action_probs = get_action_probs(np.reshape(s_prev, (1, 4)))\n",
    "        log_action_probs = log_action_probs.numpy().reshape(2,)\n",
    "        action_probs = action_probs.numpy().reshape(2,)\n",
    "        # choose an action\n",
    "        a = np.random.choice([0, 1], p=action_probs)\n",
    "        # perform a step\n",
    "        s, r, done, info = env.step(a)\n",
    "        \n",
    "        # perform a train step\n",
    "        train_step(state_previous=np.reshape(s_prev, (1, 4)), state=np.reshape(s, (1, 4)), action=a, reward=r)\n",
    "        \n",
    "        # record session history to train later\n",
    "        states = np.concatenate((states, s), axis=0)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s_prev = s.astype('float32')\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states.reshape(-1, s.shape[0]), actions, rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "policy_value_approximator = PolicyValueApproximator(action_dim=n_actions)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "Episode number 0 and the reward is 1.0\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_policy/kernel:0', 'dense_policy/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6dcc21e1165e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode number {} and the reward is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You Win!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "global_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    _, _, rewards = generate_session() # generate new sessions\n",
    "    global_rewards.append(rewards)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode number {} and the reward is {}\".format(i, np.mean(global_rewards[-100:])))\n",
    "\n",
    "    if np.mean(global_rewards[-100:]) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_approximator.save_weights('model_weights/trained_policy_actor_critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos_ac\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.24841.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_ac/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

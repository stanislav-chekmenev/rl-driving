{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)Â¶\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n",
    "\n",
    "Authors: [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0c39306fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, 'env'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyApproximator(Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(16, activation='relu', name='dense1')\n",
    "        self.d2 = Dense(32, activation='relu', name='dense2')\n",
    "        self.d3 = Dense(32, activation='relu', name='dense3')\n",
    "        self.d4 = Dense(16, activation='relu', name='dense4')\n",
    "        self.d5 = Dense(2, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        x = self.d4(x)\n",
    "        return self.d5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(states):\n",
    "    return tf.nn.softmax(policy_approximator(states))\n",
    "\n",
    "def log_policy(states):\n",
    "    return tf.nn.log_softmax(policy_approximator(states))\n",
    "\n",
    "def get_action_probs(states):\n",
    "    log_action_probs = tf.squeeze(log_policy(states), 0)\n",
    "    action_probs = tf.squeeze(policy(states), 0)\n",
    "    return log_action_probs, action_probs\n",
    "\n",
    "def get_cumulative_rewards(rewards,  gamma=0.99):\n",
    "    cumulative_returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for i in range(len(rewards) - 1, -1, -1):\n",
    "        G = gamma * G + rewards[i]\n",
    "        cumulative_returns[i] = G\n",
    "    return cumulative_returns\n",
    "\n",
    "def compute_objective(log_action_probs, actions, cumulative_returns):\n",
    "    log_action_taken_probs = tf.gather_nd(log_action_probs, tf.stack([tf.range(log_action_probs.shape[0]), actions], axis=1))\n",
    "    J = tf.math.reduce_mean(tf.math.multiply(log_action_taken_probs, cumulative_returns))\n",
    "    return J\n",
    "\n",
    "def compute_entropy(action_probs, log_action_probs):\n",
    "    return - tf.math.reduce_sum(tf.math.multiply(action_probs, log_action_probs))\n",
    "\n",
    "\n",
    "#def compute_loss(cache_action_probs, cache_log_action_probs, cumulative_returns, entropy_coef):\n",
    "#    return - compute_objective(cache_log_action_probs, cumulative_returns) - entropy_coef * entropy(cache_action_probs, cache_log_action_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions and rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards, cache_action_probs, cache_log_action_probs = np.asarray([], dtype='float32'), [], [], np.asarray([], dtype='float32'),  np.asarray([], dtype='float32')\n",
    "\n",
    "    s = env.reset().astype('float32')\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities and log probabilities\n",
    "        log_action_probs, action_probs = get_action_probs(s)\n",
    "        log_action_probs = log_action_probs.numpy().reshape(2,)\n",
    "        action_probs = action_probs.numpy().reshape(2,)\n",
    "        # choose an action\n",
    "        action_t = np.random.choice([0, 1], p=action_probs)\n",
    "        # perform a step\n",
    "        new_s, r, done, info = env.step(action_t)\n",
    "      #  print(action_probs, action_t)\n",
    "\n",
    "        # record session history to train later\n",
    "        states = np.concatenate((states, s), axis=0)\n",
    "        cache_action_probs = np.concatenate((cache_action_probs, action_probs), axis=0)\n",
    "        cache_log_action_probs = np.concatenate((cache_log_action_probs, log_action_probs), axis=0)\n",
    "        actions.append(action_t)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s.astype('float32')\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states.reshape(-1, s.shape[0]), actions, rewards, cache_action_probs.reshape(-1, env.action_space.n), cache_log_action_probs.reshape(-1, env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "policy_approximator = PolicyApproximator(action_dim=env.action_space.n)\n",
    "\n",
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    # Gradient descent step\n",
    "    with tf.GradientTape() as tape:\n",
    "        # cast everything into tensorflow tensors\n",
    "        states = tf.Variable(states, dtype='float32', name='states')\n",
    "        actions = tf.Variable(actions, dtype='int32', name='actions')\n",
    "        cumulative_returns = get_cumulative_rewards(rewards, gamma)\n",
    "        cumulative_returns = tf.Variable(cumulative_returns, dtype='float32', name='cumulative_returns')\n",
    "        \n",
    "        # Compute action probs\n",
    "        log_action_probs, action_probs = get_action_probs(states)\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy = compute_entropy(log_action_probs, action_probs)\n",
    "        \n",
    "        # Compute objective\n",
    "        #import pdb; pdb.set_trace()\n",
    "        J = compute_objective(log_action_probs, actions, cumulative_returns)\n",
    "        loss = -J + entropy_coef * entropy\n",
    "        \n",
    "       # print(policy_approximator.trainable_variables)\n",
    "    \n",
    "    gradients = tape.gradient(loss, policy_approximator.trainable_variables, )\n",
    "    optimizer.apply_gradients(zip(gradients, policy_approximator.trainable_variables))        \n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n",
      "28.14\n",
      "34.3\n",
      "35.01\n",
      "60.87\n",
      "108.74\n",
      "42.46\n",
      "226.21\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "global_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    rewards = train_on_session(*generate_session()[0:3]) # generate new sessions\n",
    "    global_rewards.append(rewards)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(np.mean(global_rewards[-100:]))\n",
    "\n",
    "    if np.mean(global_rewards[-100:]) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

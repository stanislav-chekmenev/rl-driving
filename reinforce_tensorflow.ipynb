{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)Â¶\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n",
    "\n",
    "Authors: [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f515d407e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, 'env'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyApproximator(Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(16, activation='relu', name='dense1')\n",
    "        self.d2 = Dense(32, activation='relu', name='dense2')\n",
    "        self.d3 = Dense(32, activation='relu', name='dense3')\n",
    "        self.d4 = Dense(16, activation='relu', name='dense4')\n",
    "        self.d5 = Dense(2, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        x = self.d4(x)\n",
    "        return self.d5(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "action_dim = env.action_space.n\n",
    "policy_approximator = PolicyApproximator(action_dim=action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return tf.nn.softmax(policy_approximator(state))\n",
    "\n",
    "def log_policy(state):\n",
    "    return tf.nn.log_softmax(policy_approximator(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to pick action in one given state\n",
    "def get_action_probs(s, pol):\n",
    "    if pol == 'log_policy':\n",
    "        action_probs = log_policy(s)\n",
    "    elif pol == 'policy':\n",
    "        action_probs = policy(s)\n",
    "    return tf.squeeze(action_probs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions and rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        s = tf.Variable(s, name='state_' + str(t), dtype='float32')\n",
    "        action_probs = get_action_probs(s, pol='policy').numpy().reshape(2,)\n",
    "        # choose an action\n",
    "        a = np.random.choice([0, 1], p=action_probs)\n",
    "        # perform a step\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s.numpy())\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    cumulative_returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for i in range(len(rewards) - 1, -1, -1):\n",
    "       # print(\"i step is {} now\".format(i))\n",
    "       # print('The reward after action A_{} is {}'.format(i+1, rewards[i]))\n",
    "        G = gamma * G + rewards[i]\n",
    "        cumulative_returns[i] = G\n",
    "       # print('The return after action  A_{} is {}'.format(i+1, com_returns[i]))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return tf.nn.softmax(policy_approximator(state))\n",
    "\n",
    "def log_policy(state):\n",
    "    return tf.nn.log_softmax(policy_approximator(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to pick action in one given state\n",
    "def get_action_probs(s, pol):\n",
    "    if pol == 'log_policy':\n",
    "        action_probs = log_policy(s)\n",
    "    elif pol == 'policy':\n",
    "        action_probs = policy(s)\n",
    "    return tf.squeeze(action_probs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken.\n",
    "def compute_objective(state, actions, cumulative_returns):\n",
    "    action_probs = get_action_probs(state, pol='log_policy')\n",
    "    action_taken = tf.reshape(tf.cast(actions, dtype='float32'), (state.shape[0], 1))\n",
    "    J = tf.math.reduce_mean(tf.math.multiply(cumulative_returns, action_probs))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy\n",
    "def entropy(state):\n",
    "    return -tf.math.reduce_sum(tf.math.multiply(log_policy(state), policy(state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "def compute_loss(state, actions, cumulative_returns, entropy_coef):\n",
    "    return - compute_objective(state, actions, cumulative_returns) - entropy_coef * entropy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient descent step\n",
    "    with tf.GradientTape() as tape:\n",
    "        # cast everything into tensorflow tensors\n",
    "        states = tf.Variable(states, dtype='float32', name='states')\n",
    "        actions = tf.Variable(actions, dtype='int32', name='actions')\n",
    "        cumulative_returns = get_cumulative_rewards(rewards, gamma)\n",
    "        cumulative_returns = tf.reshape(tf.Variable(cumulative_returns, dtype='float32', name='cumulative_returns'), (cumulative_returns.shape[0], 1))\n",
    "\n",
    "        # predict logits, probas and log-probas using an agent.\n",
    "        #logits = policy_approximator(states)\n",
    "        #probs = tf.nn.softmax(logits, -1)\n",
    "        #log_probs = tf.nn.log_softmax(logits, -1)\n",
    "\n",
    "        #assert all(isinstance(v, tf.Tensor) for v in [logits, probs, log_probs])\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "        # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "        loss = compute_loss(states, actions, cumulative_returns, entropy_coef)\n",
    "    \n",
    "    gradients = tape.gradient(loss, policy_approximator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, policy_approximator.trainable_variables))        \n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_session(*generate_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "21.21\n",
      "24.03\n",
      "20.87\n",
      "23.49\n",
      "22.24\n",
      "23.08\n",
      "22.33\n",
      "22.03\n",
      "22.22\n",
      "21.65\n",
      "26.8\n",
      "21.86\n",
      "21.35\n",
      "22.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-26c772cfe71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mglobal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-256abec0e01b>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# action probabilities array aka pi(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'state_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# choose an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-203-57c804812e0e>\u001b[0m in \u001b[0;36mget_action_probs\u001b[0;34m(s, pol)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'policy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9fb7c4e769b4>\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_approximator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_approximator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m     if (base_layer_utils.is_in_eager_or_tf_function() and\n\u001b[1;32m    801\u001b[0m         not call_context.in_call):\n\u001b[0;32m--> 802\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcall_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_clear_losses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1142\u001b[0m       for layer in trackable_layer_utils.filter_empty_layer_containers(\n\u001b[1;32m   1143\u001b[0m           self._layers):\n\u001b[0;32m-> 1144\u001b[0;31m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_clear_losses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_clear_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[0;34m\"\"\"Used every step in eager to reset losses.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       for layer in trackable_layer_utils.filter_empty_layer_containers(\n",
      "\u001b[0;32m~/anaconda3/envs/rlp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2248\u001b[0m           [w for w in self._non_trainable_weights if w is not existing_value])\n\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2250\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2251\u001b[0m     if (name == '_self_setattr_tracking' or\n\u001b[1;32m   2252\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_self_setattr_tracking'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_rewards = []\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    rewards = train_on_session(*generate_session()) # generate new sessions\n",
    "    global_rewards.append(rewards)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(np.mean(global_rewards[-100:]))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        CartPoleEnv\n",
       "\u001b[0;31mString form:\u001b[0m <CartPoleEnv<CartPole-v0>>\n",
       "\u001b[0;31mFile:\u001b[0m        ~/project/gym/gym/envs/classic_control/cartpole.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
       "\n",
       "Source:\n",
       "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
       "\n",
       "Observation: \n",
       "    Type: Box(4)\n",
       "    Num     Observation                 Min         Max\n",
       "    0       Cart Position             -4.8            4.8\n",
       "    1       Cart Velocity             -Inf            Inf\n",
       "    2       Pole Angle                 -24 deg        24 deg\n",
       "    3       Pole Velocity At Tip      -Inf            Inf\n",
       "    \n",
       "Actions:\n",
       "    Type: Discrete(2)\n",
       "    Num     Action\n",
       "    0       Push cart to the left\n",
       "    1       Push cart to the right\n",
       "    \n",
       "    Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
       "\n",
       "Reward:\n",
       "    Reward is 1 for every step taken, including the termination step\n",
       "\n",
       "Starting State:\n",
       "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
       "\n",
       "Episode Termination:\n",
       "    Pole Angle is more than 12 degrees\n",
       "    Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
       "    Episode length is greater than 200\n",
       "    Solved Requirements\n",
       "    Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "#assert np.allclose(get_cumulative_rewards(\n",
    "#    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "#assert np.allclose(get_cumulative_rewards(\n",
    "#    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")# That's all, thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
